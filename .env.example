# Choose your model/provider via LiteLLM's routing format:
# Examples:
# LLM_MODEL=openai/gpt-4o-mini
# LLM_MODEL=anthropic/claude-3.5-sonnet
# LLM_MODEL=groq/llama-3.1-70b-versatile
LLM_MODEL=openai/gpt-4o-mini
LLM_TEMPERATURE=0.3

# Only set the key(s) you actually need:
OPENAI_API_KEY=
ANTHROPIC_API_KEY=
GROQ_API_KEY=
